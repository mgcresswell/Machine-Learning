{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PreProcessing.ipynb","provenance":[],"authorship_tag":"ABX9TyPwrDhgo9/v7GBdxgvh/8MS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pw1IwXG1lNNr"},"source":["Mike Cresswell: Data Pre-processing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPKE5iRJlMdv","executionInfo":{"status":"ok","timestamp":1607888140259,"user_tz":360,"elapsed":981,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}},"outputId":"fd2f871c-a0d7-4e34-da8d-f8cf2668f0e2"},"source":["#Mike Cresswell\r\n","#TCSS 555\r\n","#Pre-Processing\r\n","\r\n","from __future__ import print_function\r\n","import nltk\r\n","from nltk.stem import *\r\n","nltk.download('punkt')\r\n","nltk.download('stopwords')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","nltk.download('wordnet')\r\n","from nltk.stem.porter import *\r\n","from nltk.stem.snowball import SnowballStemmer\r\n","from nltk.corpus import stopwords\r\n","from nltk.tokenize import word_tokenize\r\n","from textblob import TextBlob\r\n","from collections import defaultdict\r\n","from collections import Counter\r\n","from nltk.corpus import wordnet as wn\r\n","from nltk import pos_tag\r\n","from string import ascii_lowercase\r\n","from google.colab import files\r\n","import spacy\r\n","import requests\r\n","import gensim.downloader as api\r\n","import os\r\n","import io\r\n","import __future__\r\n","import re \r\n","import numpy\r\n","import csv\r\n","import pandas as pd\r\n","import numpy as np\r\n","import math\r\n","import sys\r\n","import itertools"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2OILz0RylXbm","executionInfo":{"status":"ok","timestamp":1607888225163,"user_tz":360,"elapsed":598,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["#Single step functions\r\n","def removeSpecialCharacterToken(words):\r\n","    somelist = [x for x in words if not IsSpecialCharacterToken(x)]\r\n","    return somelist\r\n","\r\n","def IsSpecialCharacterWord(word):\r\n","    symbols = ['`','~','!','@','#','$','%','^','&','*','(',')','_','-','+','=','{','[','}','}','|','\\\\',':',';','\"','\\'','<',',','>','.','?','/','\"\"',' ','']\r\n","    for x in symbols:\r\n","        if word == x:\r\n","            return True\r\n","    return False\r\n","\r\n","def IsSpecialCharacterToken(word):\r\n","    newWord = re.sub('[^A-Za-z0-9]+', '', word)\r\n","    if word == '':\r\n","        return True\r\n","    return False\r\n","\r\n","def checkWords(wordArray):\r\n","    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\r\n","    for x in wordArray: \r\n","        if x not in english_vocab:\r\n","            print(x)\r\n","    return words\r\n","\r\n","def stripWords(words):\r\n","    i = 0\r\n","    for x in words:   \r\n","        newWord = str.strip(x)\r\n","        newWord = str.strip(newWord, \"`~!@#$%^&*()_-+={[}}|\\\\:;\\\"<,>.?/ \")\r\n","        newWord = re.sub('[^A-Za-z0-9]+', '', newWord)\r\n","        newWord = newWord.lower()\r\n","        words[i] = newWord\r\n","        i = i + 1\r\n","    return words\r\n","\r\n","\r\n","def getWords(line):\r\n","    words = []\r\n","    for w in line.split():\r\n","        words.append(w)\r\n","    return words\r\n","\r\n","def Tokenize(words):\r\n","    words = word_tokenize(words);\r\n","    return words\r\n","\r\n","def removeStopWords(words):\r\n","    stopWords = stopwords.words('english')\r\n","    somelist = [x for x in words if not IsStopWord(x,stopWords)]\r\n","    return somelist\r\n","\r\n","def IsStopWord(word, stopWords):\r\n","    for x in stopWords:\r\n","        if word == x:\r\n","            return True\r\n","    return False\r\n","\r\n","def removeNumbers(words):\r\n","    somelist = [x for x in words if not IsNumeric(x)]\r\n","    return somelist\r\n","\r\n","def IsNumeric(word):\r\n","    if word.isnumeric():\r\n","        return True\r\n","    return False\r\n","\r\n","def get_wordnet_pos(word):\r\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n","    tag_dict = {\"J\": wn.ADJ,\r\n","                \"N\": wn.NOUN,\r\n","                \"V\": wn.VERB }\r\n","\r\n","    return tag_dict.get(tag, wordnet.NOUN)\r\n","\r\n","def lemmatizeWords(words):\r\n","    lemmatizer = WordNetLemmatizer()\r\n","    i = 0\r\n","    for x in words:           \r\n","        lemmizedWord = lemmatizer.lemmatize(x, get_wordnet_pos(x))\r\n","        words[i] = lemmizedWord\r\n","        i = i + 1\r\n","    return words\r\n","\r\n","def stemmPorterWords(words):\r\n","    stemmer = PorterStemmer()\r\n","    i = 0\r\n","    for x in words: \r\n","        stemmedWord = stemmer.stem(x)\r\n","        words[i] = stemmedWord\r\n","        i = i + 1\r\n","    return words\r\n","\r\n","\r\n","#Parent functions\r\n","#Use a number of single set functions to create cleaning/pre-processing algorithms\r\n","\r\n","#Outputs a dataset with the text lemmatized\r\n","def processTextWithLemmanization(df, outputFilePath):\r\n","    lemmantizedData = []\r\n","    for index, row in df.iterrows(): \r\n","        words = Tokenize(row['text'])\r\n","        words = stripWords(words)\r\n","        words = removeSpecialCharacterToken(words)\r\n","        words = removeStopWords(words)\r\n","        words = removeNumbers(words)\r\n","        lemmatizedWords = lemmatizeWords(words)\r\n","        newlemmatizedReview = \" \".join(lemmatizedWords)\r\n","        d = {'id': row['id'], 'deceptive': row['deceptive'], 'hotel':row['hotel'], 'polarity':row['polarity'], 'source':row['source'], 'text':newlemmatizedReview }\r\n","        lemmantizedData.append(d)\r\n","\r\n","    dfProcessedLemmatized = pd.DataFrame(data=lemmantizedData, columns=['id','deceptive','hotel','polarity','source','text'])\r\n","    dfProcessedLemmatized.to_csv(outputFilePath,index=False)\r\n","    dfProcessedLemmatized.head()\r\n","\r\n","#Outputs a dataset with the text stemmed\r\n","def processTextWithStemming(df, outputFilePath):\r\n","    stemmedData = []\r\n","    for index, row in df.iterrows(): \r\n","        words = Tokenize(row['text'])\r\n","        words = stripWords(words)\r\n","        words = removeSpecialCharacterToken(words)\r\n","        words = removeStopWords(words)\r\n","        words = removeNumbers(words)\r\n","        stemmedWords = stemmPorterWords(words)\r\n","        newStemmedReview = \" \".join(stemmedWords)\r\n","        d = {'id': row['id'], 'deceptive': row['deceptive'], 'hotel':row['hotel'], 'polarity':row['polarity'], 'source':row['source'], 'text':newStemmedReview }\r\n","        stemmedData.append(d)\r\n","    \r\n","    dfProcessedStemmed = pd.DataFrame(data=stemmedData, columns=['id','deceptive','hotel','polarity','source','text'])\r\n","    dfProcessedStemmed.to_csv(outputFilePath,index=False)\r\n","    dfProcessedStemmed.head()\r\n","\r\n","#Outputs a dataset without stemming or lemmaziation\r\n","def processText(df, outputFilePath):\r\n","    data = []\r\n","    for index, row in df.iterrows(): \r\n","        words = Tokenize(row['text'])\r\n","        words = stripWords(words)\r\n","        words = removeSpecialCharacterToken(words)\r\n","        words = removeStopWords(words)\r\n","        words = removeNumbers(words)\r\n","        words = \" \".join(words)\r\n","        d = {'id': row['id'], 'deceptive': row['deceptive'], 'hotel':row['hotel'], 'polarity':row['polarity'], 'source':row['source'], 'text':words }\r\n","        data.append(d)\r\n","    \r\n","    dfProcessed = pd.DataFrame(data=data, columns=['id','deceptive','hotel','polarity','source','text'])\r\n","    dfProcessed.to_csv(outputFilePath,index=False)\r\n","    dfProcessed.head()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Yd2qtpKlbYl","executionInfo":{"status":"ok","timestamp":1607888249247,"user_tz":360,"elapsed":21110,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["#Driver Code\r\n","#Get Dataset\r\n","url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion.csv\"\r\n","s=requests.get(url).content\r\n","df = pd.read_csv(io.StringIO(s.decode('utf-8'))) \r\n","\r\n","#Call processing algorithms\r\n","path = os.getcwd()\r\n","processTextWithLemmanization(df, path+'\\\\deceptive-opinion_lemmatized.csv')\r\n","#processTextWithStemming(df, path+'\\\\deceptive-opinion_stemmed.csv')\r\n","#processText(df, path+'\\\\deceptive-opinion_processed.csv')"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"g-EpKpOPloPh","executionInfo":{"status":"ok","timestamp":1607888207554,"user_tz":360,"elapsed":394,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}},"outputId":"621dd797-4bd7-4220-d19b-dc3f885c8b66"},"source":["files.download(path+'\\\\deceptive-opinion_lemmatized.csv') "],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_4df0cc42-3571-4ce5-b5b6-f9fb0cc9f875\", \"content\\\\deceptive-opinion_lemmatized.csv\", 824002)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]}]}