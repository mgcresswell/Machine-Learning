{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LogisticRegression.ipynb","provenance":[{"file_id":"1nYlaDD7MQlHAE1XaVIEKfzntdUnWA4Xn","timestamp":1607925820096}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oNonrin-vSLF"},"source":["Mike Cresswell: Logistic Regression\r\n"]},{"cell_type":"markdown","metadata":{"id":"zp9iFzkwvtRQ"},"source":["Classifies without the source feature:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVrqUXetEORG","executionInfo":{"status":"ok","timestamp":1607925808900,"user_tz":360,"elapsed":8409,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}},"outputId":"95a0cdb0-b702-443f-9683-98f829b43161"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import time\r\n","import io\r\n","import requests\r\n","from numpy import array\r\n","from numpy import argmax\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from sklearn.preprocessing import OneHotEncoder\r\n","from nltk.tokenize import word_tokenize\r\n","from nltk import pos_tag\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem import WordNetLemmatizer\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from collections import defaultdict\r\n","from nltk.corpus import wordnet as wn\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn.model_selection import GridSearchCV\r\n","from sklearn import model_selection, svm\r\n","from sklearn.metrics import accuracy_score\r\n","from sklearn.model_selection import StratifiedShuffleSplit\r\n","from sklearn.metrics import classification_report\r\n","from sklearn.linear_model import LogisticRegression\r\n","from sklearn.model_selection import cross_val_score\r\n","\r\n","#Get processed data\r\n","url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion_processed.csv\"\r\n","s=requests.get(url).content\r\n","Corpus = pd.read_csv(io.StringIO(s.decode('utf-8'))) \r\n","\r\n","#Get raw data for e\r\n","url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion.csv\"\r\n","s=requests.get(url).content\r\n","raw = pd.read_csv(io.StringIO(s.decode('utf-8'))) \r\n","\r\n","y = Corpus['deceptive']\r\n","#drop source feature\r\n","X = Corpus.drop(['id','deceptive','source'], axis=1)\r\n","\r\n","#feature engineering\r\n","punc = ['`','~','!','(',')','_','-','{','[','}','}',':',';','\"',',','.','?','/','\"\"']\r\n","X['char_count'] = raw[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\r\n","X['total_length'] = raw['text'].apply(len)\r\n","X['punc_count'] = raw['text'].apply(lambda x : len([a for a in x if a in punc]))\r\n","X['word_count'] = raw[\"text\"].apply(lambda x: len(str(x).split(\" \")))\r\n","X['char_count'] = raw[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\r\n","X['sentence_count'] = raw[\"text\"].apply(lambda x: len(str(x).split(\".\")))\r\n","X['avg_word_length'] = X['char_count'] / X['word_count']\r\n","X['avg_sentence_length'] = X['word_count'] / X['sentence_count']\r\n","X['word_density'] = X['word_count'] / (X['char_count'] + 1)\r\n","X['punc_count'] = raw['text'].apply(lambda x : len([a for a in x if a in punc]))\r\n","X['total_length'] = raw['text'].apply(len)\r\n","X['capitals'] = raw['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\r\n","X['num_exclamation_marks'] = raw['text'].apply(lambda x: x.count('!'))\r\n","X['num_question_marks'] = raw['text'].apply(lambda x: x.count('?'))\r\n","X['num_punctuation'] = raw['text'].apply(lambda x: sum(x.count(w) for w in '.,;:'))\r\n","X['num_symbols'] = raw['text'].apply(lambda x: sum(x.count(w) for w in '*&$%'))\r\n","X['num_unique_words'] = raw['text'].apply(lambda x: len(set(w for w in x.split())))\r\n","X['words_vs_unique'] = X['num_unique_words'] / X['word_count']\r\n","X[\"word_unique_percent\"] =  X[\"num_unique_words\"]*100/X['word_count']\r\n","\r\n","#pre preprocessing\r\n","#class label encoding\r\n","label_encoder = LabelEncoder()\r\n","y = label_encoder.fit_transform(y)\r\n","hotelEncoded = label_encoder.fit_transform(X['hotel'])\r\n","polarityEncoded = label_encoder.fit_transform(X['polarity'])\r\n","#sourceEncoded = label_encoder.fit_transform(X['source'])\r\n","\r\n","onehot_encoder = OneHotEncoder(sparse=False)\r\n","hotelEncoded = hotelEncoded.reshape(len(hotelEncoded), 1)\r\n","X['hotel'] = onehot_encoder.fit_transform(hotelEncoded)\r\n","polarityEncoded = polarityEncoded.reshape(len(polarityEncoded), 1)\r\n","X['polarity'] = onehot_encoder.fit_transform(polarityEncoded)\r\n","#sourceEncoded = sourceEncoded.reshape(len(sourceEncoded), 1)\r\n","#X['source'] = onehot_encoder.fit_transform(sourceEncoded)\r\n","\r\n","#text vectorization\r\n","Tfidf_vect = TfidfVectorizer(max_features=2301)\r\n","Tfidf_vect.fit(Corpus['text'])\r\n","X_Tfidf = Tfidf_vect.transform(X['text'])\r\n","X['text'] = X_Tfidf.toarray()\r\n","\r\n","#train/test split\r\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\r\n","for train_index, test_index in split.split(X, y):\r\n","   Train_X, Test_X = X.loc[train_index], X.loc[test_index]\r\n","   Train_Y, Test_Y = y[train_index], y[test_index]\r\n","\r\n","\r\n","model = LogisticRegression(C=1.0, max_iter=10000).fit(Train_X, Train_Y)\r\n","start = time.perf_counter()\r\n","model.fit(Train_X, Train_Y)\r\n","stop = time.perf_counter()\r\n","\r\n","#Test Accuracy \r\n","y_pred = model.predict(Test_X)\r\n","test_Accuracy = accuracy_score(Test_Y, y_pred)*100\r\n","\r\n","#Train Accuracy \r\n","y_pred = model.predict(Train_X)\r\n","train_Accuracy = accuracy_score(Train_Y, y_pred)*100\r\n","\r\n","crossvalMean = cross_val_score(model, X, y, cv=10).mean()*100\r\n","\r\n","curTime = stop - start;\r\n","print(f\"Training Time = {curTime:0.8f} Seconds\")\r\n","print(f\"Test Accuracy = {test_Accuracy}\")\r\n","print(f\"Training Accuracy = {train_Accuracy}\")\r\n","print(f\"Cross Validation Mean = {crossvalMean}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Time = 0.63021464 Seconds\n","Test Accuracy = 70.83333333333334\n","Training Accuracy = 70.0\n","Cross Validation Mean = 69.75\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hVpUmTqVv9p7"},"source":["Classifies with the source feature"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9oOtPVfD6VW","executionInfo":{"status":"ok","timestamp":1607925812412,"user_tz":360,"elapsed":811,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}},"outputId":"6ba40d71-981f-4a97-caf2-9d4f683921dc"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import time\r\n","import io\r\n","import requests\r\n","from numpy import array\r\n","from numpy import argmax\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from sklearn.preprocessing import OneHotEncoder\r\n","from nltk.tokenize import word_tokenize\r\n","from nltk import pos_tag\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem import WordNetLemmatizer\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from collections import defaultdict\r\n","from nltk.corpus import wordnet as wn\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn.model_selection import GridSearchCV\r\n","from sklearn import model_selection, svm\r\n","from sklearn.metrics import accuracy_score\r\n","from sklearn.model_selection import StratifiedShuffleSplit\r\n","from sklearn.metrics import classification_report\r\n","from sklearn.linear_model import LogisticRegression\r\n","from sklearn.model_selection import cross_val_score\r\n","\r\n","#Get Processed Data\r\n","url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion_processed.csv\"\r\n","s=requests.get(url).content\r\n","Corpus = pd.read_csv(io.StringIO(s.decode('utf-8')))\r\n","\r\n","y = Corpus['deceptive']\r\n","X = Corpus.drop(['id','deceptive'], axis=1)\r\n","\r\n","label_encoder = LabelEncoder()\r\n","y = label_encoder.fit_transform(y)\r\n","hotelEncoded = label_encoder.fit_transform(X['hotel'])\r\n","polarityEncoded = label_encoder.fit_transform(X['polarity'])\r\n","sourceEncoded = label_encoder.fit_transform(X['source'])\r\n","\r\n","onehot_encoder = OneHotEncoder(sparse=False)\r\n","hotelEncoded = hotelEncoded.reshape(len(hotelEncoded), 1)\r\n","X['hotel'] = onehot_encoder.fit_transform(hotelEncoded)\r\n","polarityEncoded = polarityEncoded.reshape(len(polarityEncoded), 1)\r\n","X['polarity'] = onehot_encoder.fit_transform(polarityEncoded)\r\n","sourceEncoded = sourceEncoded.reshape(len(sourceEncoded), 1)\r\n","X['source'] = onehot_encoder.fit_transform(sourceEncoded)\r\n","\r\n","Tfidf_vect = TfidfVectorizer(max_features=3500)\r\n","Tfidf_vect.fit(Corpus['text'])\r\n","X_Tfidf = Tfidf_vect.transform(X['text'])\r\n","X['text'] = X_Tfidf.toarray()\r\n","\r\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\r\n","for train_index, test_index in split.split(X, y):\r\n","   Train_X, Test_X = X.loc[train_index], X.loc[test_index]\r\n","   Train_Y, Test_Y = y[train_index], y[test_index]\r\n","\r\n","\r\n","model = LogisticRegression(C=1.0).fit(Train_X, Train_Y)\r\n","start = time.perf_counter()\r\n","model.fit(Train_X, Train_Y)\r\n","stop = time.perf_counter()\r\n","\r\n","#Test Accuracy \r\n","y_pred = model.predict(Test_X)\r\n","test_Accuracy = accuracy_score(Test_Y, y_pred)*100\r\n","\r\n","#Train Accuracy \r\n","y_pred = model.predict(Train_X)\r\n","train_Accuracy = accuracy_score(Train_Y, y_pred)*100\r\n","\r\n","crossvalMean = cross_val_score(model, X, y, cv=10).mean()*100\r\n","\r\n","curTime = stop - start;\r\n","print(f\"Training Time = {curTime:0.8f} Seconds\")\r\n","print(f\"Test Accuracy = {test_Accuracy}\")\r\n","print(f\"Training Accuracy = {train_Accuracy}\")\r\n","print(f\"Cross Validation Mean = {crossvalMean}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Time = 0.00500967 Seconds\n","Test Accuracy = 100.0\n","Training Accuracy = 100.0\n","Cross Validation Mean = 100.0\n"],"name":"stdout"}]}]}