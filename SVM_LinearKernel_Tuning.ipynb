{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM_LinearKernel_Tuning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPCBWNbSuUfXkYi3okcT1pr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5hwYutp3u1-s"},"source":["Mike Cresswell: Linear Tuning"]},{"cell_type":"code","metadata":{"id":"fmu0oy6Ruwru","executionInfo":{"status":"ok","timestamp":1607891367224,"user_tz":360,"elapsed":614,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["#Mike Cresswell \r\n","#TCSS 555\r\n","#SVM Linear Tuning\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","import io\r\n","import requests\r\n","import time\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from sklearn.preprocessing import OneHotEncoder\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn import svm\r\n","from sklearn.metrics import accuracy_score\r\n","from sklearn.model_selection import StratifiedShuffleSplit\r\n","from itertools import combinations"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"oY-UQlaxvCAj","executionInfo":{"status":"ok","timestamp":1607891375355,"user_tz":360,"elapsed":1716,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion_processed.csv\"\r\n","s=requests.get(url).content\r\n","Corpus = pd.read_csv(io.StringIO(s.decode('utf-8')))\r\n","\r\n","url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion.csv\"\r\n","s=requests.get(url).content\r\n","raw = pd.read_csv(io.StringIO(s.decode('utf-8')))\r\n","\r\n","y = Corpus['deceptive']\r\n","X = Corpus.drop(['id','deceptive'], axis=1)\r\n","\r\n","#feature engineering\r\n","punc = ['`','~','!','(',')','_','-','{','[','}','}',':',';','\"',',','.','?','/','\"\"']\r\n","X['char_count'] = raw[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\r\n","X['total_length'] = raw['text'].apply(len)\r\n","X['punc_count'] = raw['text'].apply(lambda x : len([a for a in x if a in punc]))\r\n","X['word_count'] = raw[\"text\"].apply(lambda x: len(str(x).split(\" \")))\r\n","X['char_count'] = raw[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\r\n","X['sentence_count'] = raw[\"text\"].apply(lambda x: len(str(x).split(\".\")))\r\n","X['avg_word_length'] = X['char_count'] / X['word_count']\r\n","X['avg_sentence_length'] = X['word_count'] / X['sentence_count']\r\n","X['word_density'] = X['word_count'] / (X['char_count'] + 1)\r\n","X['punc_count'] = raw['text'].apply(lambda x : len([a for a in x if a in punc]))\r\n","X['total_length'] = raw['text'].apply(len)\r\n","X['capitals'] = raw['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\r\n","X['num_exclamation_marks'] = raw['text'].apply(lambda x: x.count('!'))\r\n","X['num_question_marks'] = raw['text'].apply(lambda x: x.count('?'))\r\n","X['num_punctuation'] = raw['text'].apply(lambda x: sum(x.count(w) for w in '.,;:'))\r\n","X['num_symbols'] = raw['text'].apply(lambda x: sum(x.count(w) for w in '*&$%'))\r\n","X['num_unique_words'] = raw['text'].apply(lambda x: len(set(w for w in x.split())))\r\n","X['words_vs_unique'] = X['num_unique_words'] / X['word_count']\r\n","X[\"word_unique_percent\"] =  X[\"num_unique_words\"]*100/X['word_count']"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"SBsi4jTuymKp","executionInfo":{"status":"ok","timestamp":1607891378044,"user_tz":360,"elapsed":491,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["#Pre-processing\r\n","label_encoder = LabelEncoder()\r\n","y = label_encoder.fit_transform(y)\r\n","hotelEncoded = label_encoder.fit_transform(X['hotel'])\r\n","polarityEncoded = label_encoder.fit_transform(X['polarity'])\r\n","sourceEncoded = label_encoder.fit_transform(X['source'])\r\n","\r\n","onehot_encoder = OneHotEncoder(sparse=False)\r\n","hotelEncoded = hotelEncoded.reshape(len(hotelEncoded), 1)\r\n","X['hotel'] = onehot_encoder.fit_transform(hotelEncoded)\r\n","polarityEncoded = polarityEncoded.reshape(len(polarityEncoded), 1)\r\n","X['polarity'] = onehot_encoder.fit_transform(polarityEncoded)\r\n","sourceEncoded = sourceEncoded.reshape(len(sourceEncoded), 1)\r\n","X['source'] = onehot_encoder.fit_transform(sourceEncoded)\r\n","\r\n","Tfidf_vect = TfidfVectorizer(max_features=2300)\r\n","Tfidf_vect.fit(Corpus['text'])\r\n","Text_Idf = Tfidf_vect.transform(X['text'])\r\n","X['text'] = Text_Idf.toarray()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMKML3HIyqwD","executionInfo":{"status":"ok","timestamp":1607891380255,"user_tz":360,"elapsed":368,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["#Train-Test Split\r\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\r\n","for train_index, test_index in split.split(X, y):\r\n","   Train_X, Test_X = X.loc[train_index], X.loc[test_index]\r\n","   Train_Y, Test_Y = y[train_index], y[test_index]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kY3_QXD3ys-2","executionInfo":{"status":"ok","timestamp":1607892621897,"user_tz":360,"elapsed":1239299,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}},"outputId":"3c84e25d-ea90-4d37-cf5c-def2129bbc3a"},"source":["#Iterate all combinations of features\r\n","#Fit each one and export spreadshit with train time\r\n","tunningData = []      \r\n","for i in range(1, 3):\r\n","    for combo in combinations(X.columns,i): \r\n","        combo = np.array(combo)\r\n","        comboDF_TrainX = Train_X[combo]\r\n","        comboDF_TestX = Test_X[combo]\r\n","\r\n","        SVM = svm.SVC(kernel='linear')\r\n","        start = time.perf_counter()\r\n","        SVM.fit(comboDF_TrainX, Train_Y)\r\n","        end = time.perf_counter()\r\n","        y_pred = SVM.predict(comboDF_TestX)\r\n","        curTime = end - start\r\n","        row = { 'features': \",\".join(combo),'accuracy':accuracy_score(Test_Y, y_pred),'time':curTime}\r\n","        tunningData.append(row)\r\n","\r\n","#Provides a list of selected features, accuracy and training time \r\n","#Gives the best accurary with the shortest training time\r\n","#Only needed two features to classify at 100%\r\n","df = pd.DataFrame(data=tunningData, columns=['features','accuracy','time'])\r\n","df = df.sort_values(['accuracy', 'time'], ascending=[False, True])\r\n","print(df)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["                        features  accuracy       time\n","65           source,word_density   1.00000   0.001136\n","64    source,avg_sentence_length   1.00000   0.001204\n","71       source,num_unique_words   1.00000   0.001213\n","67  source,num_exclamation_marks   1.00000   0.001215\n","70            source,num_symbols   1.00000   0.001217\n","..                           ...       ...        ...\n","31                hotel,capitals   0.48750   0.169606\n","26              hotel,word_count   0.48750   0.949000\n","23              hotel,char_count   0.48750  21.437987\n","24            hotel,total_length   0.48750  44.565876\n","96       char_count,word_density   0.45625  40.710579\n","\n","[210 rows x 3 columns]\n"],"name":"stdout"}]}]}