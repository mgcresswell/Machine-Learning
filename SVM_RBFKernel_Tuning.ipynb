{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM_RBFKernel_Tuning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMtpNb96qioUzFIbDSrWrKR"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LC5mm5MR3dZ8"},"source":["Mike Cresswell: SVM RBF Tuning"]},{"cell_type":"code","metadata":{"id":"pBHpLlke3XAd","executionInfo":{"status":"ok","timestamp":1607892757526,"user_tz":360,"elapsed":2196,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["import pandas as pd\r\n","import numpy as np\r\n","import io\r\n","import requests\r\n","import time\r\n","from numpy import array\r\n","from numpy import argmax\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from sklearn.preprocessing import OneHotEncoder\r\n","from nltk.tokenize import word_tokenize\r\n","from nltk import pos_tag\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem import WordNetLemmatizer\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from collections import defaultdict\r\n","from nltk.corpus import wordnet as wn\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn.model_selection import GridSearchCV\r\n","from sklearn import model_selection, svm\r\n","from sklearn.metrics import accuracy_score\r\n","from sklearn.model_selection import StratifiedShuffleSplit\r\n","from sklearn.metrics import classification_report\r\n","from itertools import combinations"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILetBVjj4ESu","executionInfo":{"status":"ok","timestamp":1607892932921,"user_tz":360,"elapsed":560,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion_processed.csv\"\r\n","s=requests.get(url).content\r\n","Corpus = pd.read_csv(io.StringIO(s.decode('utf-8')))\r\n","\r\n","url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion.csv\"\r\n","s=requests.get(url).content\r\n","raw = pd.read_csv(io.StringIO(s.decode('utf-8')))\r\n","\r\n","y = Corpus['deceptive']\r\n","X = Corpus.drop(['id','deceptive'], axis=1)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmXptmta4zT7","executionInfo":{"status":"ok","timestamp":1607892955947,"user_tz":360,"elapsed":1602,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["#feature engineering\r\n","punc = ['`','~','!','(',')','_','-','{','[','}','}',':',';','\"',',','.','?','/','\"\"']\r\n","X['char_count'] = raw[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\r\n","X['total_length'] = raw['text'].apply(len)\r\n","X['punc_count'] = raw['text'].apply(lambda x : len([a for a in x if a in punc]))\r\n","X['word_count'] = raw[\"text\"].apply(lambda x: len(str(x).split(\" \")))\r\n","X['char_count'] = raw[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\r\n","X['sentence_count'] = raw[\"text\"].apply(lambda x: len(str(x).split(\".\")))\r\n","X['avg_word_length'] = X['char_count'] / X['word_count']\r\n","X['avg_sentence_length'] = X['word_count'] / X['sentence_count']\r\n","X['word_density'] = X['word_count'] / (X['char_count'] + 1)\r\n","X['punc_count'] = raw['text'].apply(lambda x : len([a for a in x if a in punc]))\r\n","X['total_length'] = raw['text'].apply(len)\r\n","X['capitals'] = raw['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\r\n","X['num_exclamation_marks'] = raw['text'].apply(lambda x: x.count('!'))\r\n","X['num_question_marks'] = raw['text'].apply(lambda x: x.count('?'))\r\n","X['num_punctuation'] = raw['text'].apply(lambda x: sum(x.count(w) for w in '.,;:'))\r\n","X['num_symbols'] = raw['text'].apply(lambda x: sum(x.count(w) for w in '*&$%'))\r\n","X['num_unique_words'] = raw['text'].apply(lambda x: len(set(w for w in x.split())))\r\n","X['words_vs_unique'] = X['num_unique_words'] / X['word_count']\r\n","X[\"word_unique_percent\"] =  X[\"num_unique_words\"]*100/X['word_count']"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"LhYTyVpB45XO","executionInfo":{"status":"ok","timestamp":1607892994927,"user_tz":360,"elapsed":560,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}}},"source":["#preprocessing\r\n","label_encoder = LabelEncoder()\r\n","y = label_encoder.fit_transform(y)\r\n","hotelEncoded = label_encoder.fit_transform(X['hotel'])\r\n","polarityEncoded = label_encoder.fit_transform(X['polarity'])\r\n","sourceEncoded = label_encoder.fit_transform(X['source'])\r\n","\r\n","onehot_encoder = OneHotEncoder(sparse=False)\r\n","hotelEncoded = hotelEncoded.reshape(len(hotelEncoded), 1)\r\n","X['hotel'] = onehot_encoder.fit_transform(hotelEncoded)\r\n","polarityEncoded = polarityEncoded.reshape(len(polarityEncoded), 1)\r\n","X['polarity'] = onehot_encoder.fit_transform(polarityEncoded)\r\n","sourceEncoded = sourceEncoded.reshape(len(sourceEncoded), 1)\r\n","X['source'] = onehot_encoder.fit_transform(sourceEncoded)\r\n","\r\n","Tfidf_vect = TfidfVectorizer(max_features=2300)\r\n","Tfidf_vect.fit(Corpus['text'])\r\n","Text_Idf = Tfidf_vect.transform(X['text'])\r\n","X['text'] = Text_Idf.toarray()\r\n","\r\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\r\n","for train_index, test_index in split.split(X, y):\r\n","   Train_X, Test_X = X.loc[train_index], X.loc[test_index]\r\n","   Train_Y, Test_Y = y[train_index], y[test_index]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_56wyQRr5Cmv","executionInfo":{"status":"ok","timestamp":1607893042137,"user_tz":360,"elapsed":11934,"user":{"displayName":"Mike Cresswell","photoUrl":"","userId":"05784128764127520699"}},"outputId":"67e4190b-dd26-4757-f105-42ca9ab0dd71"},"source":["tunningData = []\r\n","#rdf            \r\n","for i in range(1, 3):\r\n","    for combo in combinations(X.columns,i): \r\n","        combo = np.array(combo)\r\n","        comboDF_TrainX = Train_X[combo]\r\n","        comboDF_TestX = Test_X[combo]\r\n","\r\n","        SVM = svm.SVC(kernel='rbf', gamma='scale')\r\n","        start = time.perf_counter()\r\n","        SVM.fit(comboDF_TrainX, Train_Y)\r\n","        end = time.perf_counter()\r\n","        y_pred = SVM.predict(comboDF_TestX)\r\n","        curTime = end - start\r\n","        row = { 'features': \",\".join(combo),'accuracy':accuracy_score(Test_Y, y_pred),'time':curTime}\r\n","        tunningData.append(row)\r\n","\r\n","\r\n","df = pd.DataFrame(data=tunningData, columns=['features','accuracy','time'])\r\n","df = df.sort_values(['accuracy', 'time'], ascending=[False, True])\r\n","print(df)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["                           features  accuracy      time\n","57                      source,text  1.000000  0.001494\n","65              source,word_density  1.000000  0.001535\n","39                  polarity,source  1.000000  0.001693\n","21                     hotel,source  1.000000  0.001724\n","2                            source  1.000000  0.001878\n","..                              ...       ...       ...\n","104  char_count,word_unique_percent  0.485417  0.043151\n","12                         capitals  0.483333  0.040514\n","5                      total_length  0.483333  0.041629\n","4                        char_count  0.481250  0.041968\n","7                        word_count  0.479167  0.043675\n","\n","[210 rows x 3 columns]\n"],"name":"stdout"}]}]}